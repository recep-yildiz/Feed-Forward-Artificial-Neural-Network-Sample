#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <alloc.h>
#include <time.h>
#include <conio.h>

#define LIMIT 22
#define ITERATION 1E6

#define ETA 0.010
#define MOMENTUM 0.600
#define GAMMA 0.0001
#define BETA 0.050

#define d(i, j)    *(po + ((j)-1)*LIMIT+(i))
#define x(i, j)    *(pi + ((j)-1)*LIMIT+(i))

#define TOLERANCE 0.000001

//FUNCTIONS

void getStructureInfo(void);
void assignInitialValues(void);
void computeForwardValues(void);
void loadTrainingPairs(void);
void showTrainingPairs(void);
void showOutputs(void);
void computeOutputDeltaValues(int sample);
void computeHiddenDeltaValues(void);
void updateParameters(float eta);
void saveTrainedNetworkParameters(int counter);

float nonLinearFunction(float sum, float threshold);
float derivativeNonlinearFunction(float sum);


// GLOBAL VARIABLES

struct WEIGHTS
{
    float w[LIMIT][LIMIT];
    float w1[LIMIT][LIMIT];
    float w2[LIMIT][LIMIT];
}w[5];

struct VECTOR
[
    float out[LIMIT];     //output values
    float del[LIMIT];     //delta values
    float thr[LIMIT];     //threshold values
    float SJ[LIMIT];
}s[5];

int L, layer [LIMIT+1];
int PAIRS, BACKUP = 1E3;
float far *pi, *po;
char train_pairs[20];
char file_name[20];
FILE *fp, *fer;

void main()
{
    int counter, sample, k, i, choi;
    float cluster_error, old_cluster_error, eta = ETA, error_sample;
    
    pi = (float far * ) farmalloc ((long) (5000));
    if(pi == NULL)
    {printf ("\n Out of Memory... \n"); exit(1);}
    
    clrscr();
    getStructureInfo();
    assignInitialValues();
    loadTrainingPairs();
    showTrainingPairs();
    
    if((fer = fopen ("ERROR.M", "w")) == NULL)
    {
        fprintf(stderr, " ERROR.M File could not open\n");
        exit(1);
    }
    
    clrscr();
    old_cluster_error = 0;
    for(counter = 0; counter <= ITERATION; counter++)
    {
        cluster_error = 0;
        for(sample = 1; sample <= PAIRS; sample++)
        {
            error_sample = 0;
            for(k = 1; k <= layer[0]; k++)
                s[0].out[k] = x(k, sample);
            computeForwardValues();
            showOutputs(sample);
            
            for(i = 1; i <= layer[L+1]; i++)
                error_sample = error_sample + 0.5 * pow ((d(i, sample)-s[L+1].out[i]), 2);
                
            cluster_error += error_sample;
            computeOutputDeltaValues(sample);
            computeHiddenDeltaValues();
            updateParameters(eta);
        }
        
        fprintf(fer, " %d  %f \n", counter, cluster_error);
        
        //Step Size Adaptation
        if(cluster_error - old_cluster_error < 0) 
            eta += GAMMA;
        else if(cluster_error - old_cluster_error > 0)
            eta = (1-BETA)*eta;
        else{//NO CHANGE}
        
        old_cluster_error = cluster_error;
        
        gotoxy(20, 20);
        printf("Cluster Error = %f ", cluster_error);
        
        gotoxy(20, 21);
        printf("Number of Cluster Transitions = %d", counter);
    }
    
    // SAVING NETWORK PARAMETERS (OUTPUT)
    if( counter < ITERATION && ( kbhit() || cluster_error <=  TOLERANCE))
    {
        saveTrainedNetworkParameters(counter);
        gotoxy(20,23);
        printf("Latest Status Saved.");
        gotoxy(12,24);
        printf("Check the <&s> file for details.\n, file_name");
        break;
    }
    
    // BACKUP NETWORK PARAMETERS
    if( (counter%BACKUP) == (BACKUP -1))
    {
        gotoxy(20,23);
        printf("Saving automatically...      ");
        saveTrainedNetworkParameters(counter);
        gotoxy(20,23);
        printf("  ");
    }
}

    fclose(fer);
    farfree(pi);
    farfree(po);
}

//**************************************************************************
void getStructureInfo(void)
{
    int i;
    printf("\n\n Training Pairs File Name: ");
    scanf("%s", train_pairs);
    printf("\n\n Enter Number of Hidden Layers: ");
    scanf("%d", &L);

    // (L+1)th layer is the output layer
    // Zero layer is the input layer
   
   printf("Enter Number of Input Neurons: ");
   scanf("%d", &layer[0]);
   printf("Enter Number of Output Neurons: ");
   scanf("%d", &layer[L+1]);
   
   for(i=1; i<=L; i++)
   {
       printf("Enter Number of Neurons at &d. Layer: ", i);
       scanf("%d", &layer[i]);
   }
   gotoxy(1,16);
   printf("Enter File Name of Trained Network Parameters to Save: ");
   scanf("%s", file_name);
}

//**************************************************************************
void assignInitialValues(void)
{
    int k, i, j;
    
    randomize();
    for(k=0; k<=L; k++)
    {
        for(i=1; i<=layer[k+1]; i++)
        {
            for(j=1; j<=layer[k]; j++)
            {
                W[k].w[i][j] = ((float) (rand()%10000))/10000-0.5;
                W[k].w1[i][j] = ((float) (rand()%10000))/10000-0.5;
                W[k].w2[i][j] = ((float) (rand()%10000))/10000-0.5;
            }
            s[k].thr[i] = ((float) (rand()%10000))/100000-0.5;
        }
    }
}

//**************************************************************************
float nonLinearFunction(float sum, float threshold)
{
    float value;
    value = tanhl(sum - threshold);         //tanhl
    return(value);
}

//**************************************************************************
void computeForwardValues(void)
{
    int i, j, k;
    float sum;
    
    for(k=0; k<=L; k++)
    {
        for(i=1; i<=layer[k+1]; i++)
        {
            sum = 0;
            for(j=1; j<=layer[k]; j++)
            {
                sum = sum + W[k].w[i][j] * s[k].out[j];
            }
            s[k+1].SJ[i] = sum;
            if(k!=L) 
                s[k].out[i] = nonLinearFunction(s[k+1].SJ[i], s[k+1].thr[i]);
            else
                s[k+1].out[i] = sum - s[k+1].thr[i];
        }
    }
}

//**************************************************************************
void derivativeNonlinearFunction(float sum)
{
    float derivative;
    derivative = (1 - sum*sum);
    return(derivative);
}

//**************************************************************************
void loadTrainingPairs(void)
{
    int i, j;
    float temp;
    
    // Load Training Pairs to Memory
    // Every Single Consecutive Layer[0] -elements Groups is one Train
    // Constitute Input of Pair 
    // x(<which input of network>,<which sample(in order)>)
    // d(<which output of network>, <which sample(in order)>)
    
    if((fp = fopen( train_pairs, "r")) == NULL)
    {
        fprintf(stderr, "File Can Not Open: %s\n", train_pairs);
        exit(1);
    }
    fscanf(fp, "%d", &PAIRS);
    
    for(j=1; j<=PAIRS; j++)
    {
        for(i=1; i<=layer[0]; i++)
        {
            fscanf(fp, "%f", &temp);
            d(i, j) = temp;
        }
    }
    fclose(fp);
}

//**************************************************************************
void showTrainingPairs(void)
{
    int i, j;
    
    clrscr();
    for(j=1; j<=PAIRS; j++)
    {
        for(i=1; i<=layer[0]; i++)
        {
            printf("x(%d, %d) = %f ", i, j, x(i, j));
        }
        for(i=1; i<=layer[L+1]; i++)
        {
            print("d(%d, %d) = %f\n", i, j, d(i, j));
        }
    }
    getch();
}

//**************************************************************************
void showOutputs(int sample)
{
    int i, j;
    
    for(i=1; i<=PAIRS; i++)
    {
        gotoxy(5, 1 + sample%15);
        for(j=1; j<=layer[L+1]; j++)
        {
            printf("out = %f", s[L+1].out[1]);
        }
    }
}

//**************************************************************************
void computeOutputDeltaValues(int sample)
{
    
}

//**************************************************************************
void computeHiddenDeltaValues(void)
{

}

//**************************************************************************
void updateParameters(float eta)
{

}

//**************************************************************************
void saveTrainedNetworkParameters(int counter)
{

}
